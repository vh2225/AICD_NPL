{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Named Entity Recognition - demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's start by importing all the important classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dropout, TimeDistributed, Bidirectional, \\\n",
    "    LSTM, concatenate, Dense\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.utils import to_categorical\n",
    "from nlp_architect.data.sequential_tagging import SequentialTaggingDataset\n",
    "from nlp_architect.utils.embedding import load_word_embeddings\n",
    "from nlp_architect.utils.metrics import get_conll_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Load the dataset using the `NamedEntityDataset` data loader.\n",
    "The files should be tagged in `BIO` format and each token should appear in a separate line with its tags separated by tabs. For example: `A B-ENTITY`.  Sentence should be separated by an empty line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = '<path to train file>'\n",
    "test = '<path to train file>'\n",
    "\n",
    "sentence_length = 50\n",
    "word_length = 12\n",
    "\n",
    "dataset = SequentialTaggingDataset(train, test,\n",
    "                             max_sentence_length=sentence_length,\n",
    "                             max_word_length=word_length,\n",
    "                             tag_field_no=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the train and test sets - we have 2 inputs and 1 output (word and chars, and entity type for outout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_char_train, y_train = dataset.train\n",
    "x_test, x_char_test, y_test = dataset.test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert output matrices into 1-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_y_labels = len(dataset.y_labels) + 1\n",
    "y_test = to_categorical(y_test, num_y_labels)\n",
    "y_train = to_categorical(y_train, num_y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading external word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_path = '/<path to glove.6B>/glove.6B.100d.txt'\n",
    "embedding_size = 100\n",
    "\n",
    "external_emb, emb_size = load_word_embeddings(embedding_path)\n",
    "embedding_matrix = np.zeros((dataset.word_vocab_size, emb_size))\n",
    "for word, i in dataset.word_vocab.items():\n",
    "    embedding_vector = external_emb.get(word.lower())\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "The NER model we're going to build is depicted below:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We have 2 input source (words and word characters), a bi-directional LSTM layer and a CRF layer for token classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vocab_size = dataset.word_vocab_size\n",
    "char_vocab_size = dataset.char_vocab_size\n",
    "num_y_labels = len(dataset.y_labels) + 1\n",
    "char_embedding_dims = 25\n",
    "word_lstm_dims = 25\n",
    "tagger_lstm_dims = 100\n",
    "\n",
    "# build word input\n",
    "words_input = Input(shape=(sentence_length,), name='words_input')\n",
    "\n",
    "embedding_layer = Embedding(word_vocab_size,\n",
    "                            embedding_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=sentence_length,\n",
    "                            trainable=False)\n",
    "\n",
    "word_embeddings = embedding_layer(words_input)\n",
    "word_embeddings = Dropout(0.5)(word_embeddings)\n",
    "\n",
    "# create word character embeddings\n",
    "word_chars_input = Input(shape=(sentence_length, word_length), name='word_chars_input')\n",
    "char_embedding_layer = Embedding(char_vocab_size, char_embedding_dims,\n",
    "                                 input_length=word_length)\n",
    "char_embeddings = TimeDistributed(char_embedding_layer)(word_chars_input)\n",
    "char_embeddings = TimeDistributed(Bidirectional(LSTM(word_lstm_dims)))(char_embeddings)\n",
    "char_embeddings = Dropout(0.5)(char_embeddings)\n",
    "\n",
    "# create the final feature vectors\n",
    "features = concatenate([word_embeddings, char_embeddings], axis=-1)\n",
    "\n",
    "# encode using a bi-lstm\n",
    "bilstm = Bidirectional(LSTM(tagger_lstm_dims, return_sequences=True))(features)\n",
    "bilstm = Dropout(0.5)(bilstm)\n",
    "\n",
    "# classify the dense vectors\n",
    "crf = CRF(num_y_labels, sparse_target=False)\n",
    "predictions = crf(bilstm)\n",
    "\n",
    "# compile the model\n",
    "model = Model(inputs=[words_input, word_chars_input], outputs=predictions)\n",
    "model.compile(loss=crf.loss_function,\n",
    "              optimizer='adam',\n",
    "              metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "set batch size and number of epochs and fit the data on the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = 32\n",
    "e = 1\n",
    "\n",
    "model.fit(x=[x_train, x_char_train], y=y_train,\n",
    "              batch_size=b,\n",
    "              epochs=e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Once the model has trained. Run CONLLEVAL to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict([x_test, x_char_test], batch_size=b)\n",
    "\n",
    "eval = get_conll_scores(predictions, y_test, {v: k for k, v in dataset.y_labels.items()})\n",
    "print('Precision {}'.format(eval[0][0]))\n",
    "print('Recall {}'.format(eval[0][1]))\n",
    "print('F1 {}'.format(eval[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
